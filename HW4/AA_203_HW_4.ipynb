{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AA 203 HW 4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgOQfaa6fUvvLwnS9rGqiO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmoy14-stanford/AA203-Homework/blob/master/HW4/AA_203_HW_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkM7_nlD0vZE"
      },
      "source": [
        "Getting OpenAI Gym to work on Colab:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZhr98Fj0oJT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1735f333-24a1-45a3-e066-d2fa467b3cc7"
      },
      "source": [
        "%%bash \n",
        "# install required system dependencies\n",
        "apt-get install -y xvfb x11-utils \n",
        "apt-get install swig\n",
        "# install required python dependencies\n",
        "pip install gym[box2d]==0.17.* \\\n",
        "            pyvirtualdisplay==0.2.* \\\n",
        "            PyOpenGL==3.1.* \\\n",
        "            PyOpenGL-accelerate==3.1.* \\\n",
        "            pyglet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "x11-utils is already the newest version (7.7+3build1).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n",
            "Requirement already satisfied: gym[box2d]==0.17.* in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyvirtualdisplay==0.2.* in /usr/local/lib/python3.7/dist-packages (0.2.5)\n",
            "Requirement already satisfied: PyOpenGL==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: PyOpenGL-accelerate==3.1.* in /usr/local/lib/python3.7/dist-packages (3.1.5)\n",
            "Requirement already satisfied: pyglet in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (1.4.1)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.7/dist-packages (from gym[box2d]==0.17.*) (2.3.8)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.7/dist-packages (from pyvirtualdisplay==0.2.*) (0.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmGo7HaD09WN"
      },
      "source": [
        "import pyvirtualdisplay\n",
        "# use False with Xvfb\n",
        "_display = pyvirtualdisplay.Display(visible=False,  \n",
        "                                    size=(1400, 900))\n",
        "_ = _display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qN9cc_IV1_D3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "e479f5cd-9803-4be1-d0d9-fc5b9352a6f3"
      },
      "source": [
        "#%%\n",
        "import argparse\n",
        "import gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cart Pole\n",
        "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
        "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
        "                    help='discount factor (default: 0.99)')\n",
        "parser.add_argument('--seed', type=int, default=203, metavar='N',\n",
        "                    help='random seed (default: 203)')\n",
        "parser.add_argument('--render', default=True,\n",
        "                    help='render the environment')\n",
        "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
        "                    help='interval between training status logs (default: 10)')\n",
        "parser.add_argument(\"-f\", \"--file\", required=False) \n",
        "\n",
        "# based on:\n",
        "# https://github.com/pytorch/examples/blob/master/reinforcement_learning/actor_critic.py\n",
        "\n",
        "args = parser.parse_args()\n",
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "env.reset()\n",
        "img = plt.imshow(env.render('rgb_array')) \n",
        "\n",
        "env.seed(args.seed)\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    \"\"\"\n",
        "    implements both actor and critic in one model\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(state_dim, 128)\n",
        "        self.affine2 = nn.Linear(128, 128)\n",
        "\n",
        "        # actor's layer\n",
        "        self.action_mean = nn.Linear(128, action_dim)\n",
        "        self.action_var = nn.Linear(128, action_dim)\n",
        "        # critic's layer\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "        # action & reward buffer\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        forward of both actor and critic\n",
        "        \"\"\"\n",
        "        # TODO map input to \n",
        "        # mean of action distribution\n",
        "        # variance of action distribution (pass this through a non-negative function), e.g. torch.exp()\n",
        "        # state value\n",
        "\n",
        "        x = F.relu(self.affine2(F.relu(self.affine1(x))))\n",
        "\n",
        "        action_mean = self.action_mean(x)\n",
        "        action_var = torch.exp(self.action_var(x))\n",
        "        state_values = self.value_head(x)\n",
        "        \n",
        "        return 0.5*action_mean, 0.5*action_var, state_values\n",
        "    \n",
        "model = Policy().float()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "def select_action(state):\n",
        "    state = torch.from_numpy(state).float()\n",
        "    mu, sigma, state_value = model(state)\n",
        "    \n",
        "    # create a normal distribution over the continuous action space\n",
        "    m = Normal(loc=mu,scale=sigma)\n",
        "    \n",
        "    # and sample an action using the distribution\n",
        "    action = m.sample()\n",
        "    \n",
        "    # save to action buffer\n",
        "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "    \n",
        "    # the action to take (left or right)\n",
        "    return action.data.numpy()\n",
        "\n",
        "def finish_episode():\n",
        "    \"\"\"\n",
        "    Training code. Calculates actor and critic loss and performs backprop.\n",
        "    \"\"\"\n",
        "    R = 0\n",
        "    saved_actions = model.saved_actions\n",
        "    policy_losses = [] # list to save actor (policy) loss\n",
        "    value_losses = [] # list to save critic (value) loss\n",
        "    returns = [] # list to save the true values\n",
        "    \n",
        "    # calculate the true value using rewards returned from the environment\n",
        "    for r in model.rewards[::-1]:\n",
        "        # TODO compute the value at state x\n",
        "        # via the reward and the discounted tail reward\n",
        "        R = r + args.gamma * R\n",
        "        \n",
        "        \n",
        "        returns.insert(0, R)\n",
        "        \n",
        "    # whiten the returns\n",
        "    returns = torch.tensor(returns).float()\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "    \n",
        "    for (log_prob, value), R in zip(saved_actions, returns):\n",
        "        # TODO compute the advantage via subtracting off value\n",
        "        advantage = R - value.item()  \n",
        "        \n",
        "        # TODO calculate actor (policy) loss, from log_prob (saved in select action)\n",
        "        # and from advantage\n",
        "        # append this to policy_losses\n",
        "        policy_losses.append(-log_prob * advantage)\n",
        "        \n",
        "        # TODO calculate critic (value) loss\n",
        "        value_losses.append(F.mse_loss(value, torch.tensor([R])))\n",
        "\n",
        "        \n",
        "    # reset gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # sum up all the values of policy_losses and value_losses\n",
        "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
        "    \n",
        "    # perform backprop\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # reset rewards and action buffer\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]\n",
        "\n",
        "def main():\n",
        "    running_reward = -100\n",
        "    \n",
        "    # run infinitely many episodes, until performance criteria met\n",
        "    episodic_rewards = []\n",
        "    \n",
        "    for i_episode in count(1):\n",
        "        # reset environment and episode reward\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "\n",
        "        for t in range(1, 2500):\n",
        "            # select action from policy\n",
        "            action = select_action(state)\n",
        "            \n",
        "            # take the action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            if args.render and i_episode % 100 == 0:\n",
        "                  screen = env.render(mode='rgb_array')\n",
        "\n",
        "                  plt.imshow(screen)\n",
        "                  ipythondisplay.clear_output(wait=True)\n",
        "                  ipythondisplay.display(plt.gcf())\n",
        "    \n",
        "            model.rewards.append(reward)\n",
        "            ep_reward += reward\n",
        "            if done:\n",
        "                episodic_rewards.append(ep_reward)\n",
        "                break\n",
        "                \n",
        "        # update cumulative reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "        \n",
        "        # perform backprop\n",
        "        finish_episode()\n",
        "        \n",
        "        # log results\n",
        "        if i_episode % args.log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                  i_episode, ep_reward, running_reward))\n",
        "            \n",
        "        # check if we have \"solved\" the problem\n",
        "        if running_reward > 200:\n",
        "            print(\"Solved! Running reward is now {} and \"\n",
        "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
        "\n",
        "            # TODO plot episodic_rewards --- submit this plot with your code\n",
        "            fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))\n",
        "            ax1.set_xlabel(\"Episode\")\n",
        "            ax1.set_ylabel(\"Reward\")\n",
        "            # ax1.set_title(\"ESS Revenue, Disaggregated\")\n",
        "            # p1 = ax1.plot(times_plt, lmp_ls)\n",
        "            # p2 = ax1.plot(times_plt, -tou_ls)\n",
        "            ax1.set_title(\"Episodic Rewards\")\n",
        "            p1 = ax1.plot(episodic_rewards)\n",
        "            plt.grid()\n",
        "            plt.savefig(\"ep_rewards.png\", dpi=400)\n",
        "            break\n",
        "            \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-8d8457397545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLanderContinuous-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVIEWPORT_W\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVIEWPORT_H\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mcompat_platform\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'darwin'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcocoa\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCocoaConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m  \u001b[0;31m# noqa: F821\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eivG4W-rnOVC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}